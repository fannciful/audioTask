import torch
import json
import pandas as pd
from sklearn.metrics import classification_report, confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

def validate_model(model_path):
    """–ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ —è–∫–æ—Å—Ç—ñ –º–æ–¥–µ–ª—ñ –Ω–∞ —Å—Ç–∞–±—ñ–ª—å–Ω–∏—Ö –∑—Ä–∞–∑–∫–∞—Ö"""
    print("üîç Validating model...")
    
    # –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –º–æ–¥–µ–ª—ñ
    model = torch.load(model_path, map_location='cpu')
    
    # –¢–µ—Å—Ç–æ–≤—ñ –¥–∞–Ω—ñ –¥–ª—è –≤–∞–ª—ñ–¥–∞—Ü—ñ—ó
    test_samples = generate_validation_samples()
    
    # –ü—Ä–æ–≥–Ω–æ–∑—É–≤–∞–Ω–Ω—è
    predictions = []
    true_labels = []
    
    for sample, true_label in test_samples:
        pred = model(sample.unsqueeze(0))
        pred_label = torch.argmax(pred).item()
        predictions.append(pred_label)
        true_labels.append(true_label)
    
    # –ú–µ—Ç—Ä–∏–∫–∏ —è–∫–æ—Å—Ç—ñ
    report = classification_report(true_labels, predictions, output_dict=True)
    cm = confusion_matrix(true_labels, predictions)
    
    # –ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤
    results = {
        'accuracy': report['accuracy'],
        'precision': report['weighted avg']['precision'],
        'recall': report['weighted avg']['recall'],
        'f1_score': report['weighted avg']['f1-score'],
        'confusion_matrix': cm.tolist()
    }
    
    with open('artifacts/validation_metrics.json', 'w') as f:
        json.dump(results, f, indent=2)
    
    # –ó–≤—ñ—Ç —É HTML
    generate_html_report(report, cm)
    
    print(f"‚úÖ Validation completed. Accuracy: {results['accuracy']:.3f}")

def generate_validation_samples():
    """–ì–µ–Ω–µ—Ä–∞—Ü—ñ—è —Å—Ç–∞–±—ñ–ª—å–Ω–∏—Ö —Ç–µ—Å—Ç–æ–≤–∏—Ö –∑—Ä–∞–∑–∫—ñ–≤"""
    # –¢—É—Ç –º–∞—é—Ç—å –±—É—Ç–∏ —Ä–µ–∞–ª—å–Ω—ñ —Ç–µ—Å—Ç–æ–≤—ñ –¥–∞–Ω—ñ
    return []  # –ó–∞–≥–ª—É—à–∫–∞

if __name__ == "__main__":
    import argparse
    parser = argparse.ArgumentParser()
    parser.add_argument('--model-path', required=True)
    args = parser.parse_args()
    
    validate_model(args.model_path)