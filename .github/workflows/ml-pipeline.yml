name: ML Pipeline for Model Lifecycle Management

on:
  # Manual execution from GitHub interface
  workflow_dispatch:
    inputs:
      training_cycles:
        description: 'Training cycles count'
        required: true
        default: '3'
        type: string
      compact_dataset:
        description: 'Use compact dataset'
        required: false
        default: true
        type: boolean
      upload_to_registry:
        description: 'Upload to Container Registry'
        required: false
        default: false
        type: boolean

  # Merge request validation
  pull_request:
    branches: [ primary, main, development ]
    types: [opened, synchronize, reopened, ready_for_review]
  
  # Automatic deployment on main branch
  push:
    branches: [ primary, main ]
    paths-ignore:
      - '*.md'
      - 'documentation/**'
      - 'docs/**'
      - 'README*'

env:
  REGISTRY_URL: ghcr.io
  REPOSITORY_NAME: ${{ github.repository }}
  ARTIFACT_NAME: ml-model-${{ github.run_id }}

jobs:
  # Job 1: Model training process
  train-model:
    name: Train ML Model
    runs-on: ubuntu-22.04
    timeout-minutes: 40
    
    outputs:
      model_version: ${{ steps.create-tag.outputs.model-version }}
      training_status: ${{ steps.execute-training.outputs.training-status }}
      performance_score: ${{ steps.execute-training.outputs.performance-score }}
    
    steps:
    - name: Download repository
      uses: actions/checkout@v4
    
    - name: Optimize workspace
      run: |
        echo "Optimizing system resources..."
        sudo apt-get autoremove -y
        sudo apt-get clean
        docker system prune -a -f --volumes 2>/dev/null || true
        echo "Storage status:"
        df -h
    
    - name: Configure Docker
      uses: docker/setup-buildx-action@v3
    
    - name: Generate model identifier
      id: create-tag
      run: |
        TIMESTAMP=$(date +%Y%m%d-%H%M%S)
        echo "model-version=model-v${TIMESTAMP}-${GITHUB_SHA:0:8}" >> $GITHUB_OUTPUT
        echo "Generated identifier: model-v${TIMESTAMP}-${GITHUB_SHA:0:8}"
    
    - name: Create training environment
      run: |
        echo "Creating training environment..."
        docker build -f Dockerfile.train -t model-trainer:latest .
    
    - name: Run training process
      id: execute-training
      run: |
        echo "Starting model training..."
        
        # Prepare output location
        mkdir -p model-output
        
        # Execute training
        docker run --rm \
          -v $(pwd)/model-output:/app/artifacts \
          -v $(pwd)/data:/app/data \
          -e EPOCHS=${{ github.event.inputs.training_cycles || '3' }} \
          -e SAMPLES_PER_CLASS=${{ github.event.inputs.compact_dataset && '10' || '50' }} \
          model-trainer:latest
        
        # Check for successful training
        echo "Checking for model files..."
        echo "Files in model-output directory:"
        ls -la model-output/ || true
        
        if [ -f "model-output/model.pth" ] || [ -f "model-output/model_full.pth" ]; then
          echo "Training completed successfully - model files found"
          echo "training-status=success" >> $GITHUB_OUTPUT
          
          # Try to extract performance score from training_metrics.json
          if [ -f "model-output/training_metrics.json" ]; then
            ACCURACY=$(python3 -c "
import json
try:
    with open('model-output/training_metrics.json', 'r') as f:
        data = json.load(f)
    print(data.get('final_metrics', {}).get('test_accuracy', '25.0'))
except:
    print('25.0')
" 2>/dev/null || echo "25.0")
            echo "performance-score=$ACCURACY" >> $GITHUB_OUTPUT
            echo "Model Accuracy: $ACCURACY%"
          elif [ -f "model-output/training.log" ]; then
            # Look for accuracy in the log file
            ACCURACY=$(grep -i "Final Accuracy" model-output/training.log | grep -o '[0-9]*\.[0-9]*' | head -1 || echo "25.0")
            echo "performance-score=$ACCURACY" >> $GITHUB_OUTPUT
            echo "Model Accuracy: $ACCURACY%"
          else
            # Default value
            echo "performance-score=25.0" >> $GITHUB_OUTPUT
            echo "Model Accuracy: 25.0% (default)"
          fi
        else
          echo "Training failed - no model files found"
          echo "Available files:"
          find model-output -type f 2>/dev/null || true
          echo "training-status=failed" >> $GITHUB_OUTPUT
          exit 1
        fi
    
    - name: Create inference environment
      run: |
        echo "Creating inference environment..."
        docker build -f Dockerfile.inference -t model-server:latest .
    
    - name: Save training artifacts
      uses: actions/upload-artifact@v4
      with:
        name: ${{ env.ARTIFACT_NAME }}
        path: |
          model-output/
        retention-days: 28

  # Job 2: Model validation
  validate-model:
    name: Validate Model Quality
    runs-on: ubuntu-22.04
    needs: train-model
    if: needs.train-model.outputs.training_status == 'success'
    
    steps:
    - name: Download repository
      uses: actions/checkout@v4
    
    - name: Get model artifacts
      uses: actions/download-artifact@v4
      with:
        name: ${{ env.ARTIFACT_NAME }}
        path: model-output/
    
    - name: Setup Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
    
    - name: Install dependencies
      run: |
        echo "Installing validation dependencies..."
        pip install torch torchaudio numpy
    
    - name: Check available files
      run: |
        echo "Available files in model-output:"
        ls -la model-output/ || true
        echo "Looking for model files..."
        find model-output -name "*.pth" -type f 2>/dev/null || echo "No .pth files found"
    
    - name: Execute validation
      run: |
        echo "Running model validation..."
        
        # Create a simple validation script
        cat > validate_model.py << 'EOF'
    import torch
    import json
    import os
    import sys

print('Checking model directory...')

# Check for different possible model file names
model_files = ['model-output/model.pth', 'model-output/model_full.pth']
model_path = None

# First, list all files
print('Files in model-output:')
for root, dirs, files in os.walk('model-output'):
    for file in files:
        print(f'  {os.path.join(root, file)}')
        if file.endswith('.pth'):
            model_path = os.path.join(root, file)

if model_path is None:
    print('ERROR: No model file found')
    print('Current directory contents:')
    os.system('find . -name "*.pth" -type f 2>/dev/null || echo "No .pth files"')
    sys.exit(1)

print(f'Loading model from: {model_path}')

try:
    # Load class info
    class_info_path = 'model-output/class_info.json'
    if os.path.exists(class_info_path):
        with open(class_info_path, 'r') as f:
            class_info = json.load(f)
            target_classes = class_info.get('target_classes', ['yes', 'no', 'up', 'down'])
    else:
        target_classes = ['yes', 'no', 'up', 'down']
    
    print(f'Target classes: {target_classes}')
    
    # Try to load the model
    device = torch.device('cpu')
    
    # First, try to load as complete model
    try:
        model = torch.load(model_path, map_location=device)
        print('Model loaded as complete object')
        
        # Check if it's actually a model object
        if not hasattr(model, 'eval'):
            print('Loaded object is not a model, trying state dict...')
            raise ValueError('Not a model')
            
    except Exception as e1:
        print(f'Complete model load failed: {e1}')
        
        # Try to load as state dict - define the model architecture first
        class AudioClassifier(torch.nn.Module):
            def __init__(self, num_classes=4):
                super().__init__()
                self.conv1 = torch.nn.Conv2d(1, 8, kernel_size=3, stride=1, padding=1)
                self.conv2 = torch.nn.Conv2d(8, 16, kernel_size=3, stride=1, padding=1)
                self.conv3 = torch.nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)
                self.pool = torch.nn.MaxPool2d(2)
                self.relu = torch.nn.ReLU()
                self.dropout = torch.nn.Dropout(0.3)
                self.gap = torch.nn.AdaptiveAvgPool2d((4, 2))
                self.fc1 = torch.nn.Linear(32 * 4 * 2, 64)
                self.fc2 = torch.nn.Linear(64, num_classes)
            
            def forward(self, x):
                x = self.pool(self.relu(self.conv1(x)))
                x = self.pool(self.relu(self.conv2(x)))
                x = self.pool(self.relu(self.conv3(x)))
                x = self.gap(x)
                x = x.view(x.size(0), -1)
                x = self.relu(self.fc1(self.dropout(x)))
                x = self.fc2(self.dropout(x))
                return x
        
        model = AudioClassifier(num_classes=len(target_classes))
        model.load_state_dict(torch.load(model_path, map_location=device))
        print('Model loaded via state dict')
    
    model.eval()
    
    print('Running validation tests...')
    test_results = []
    
    for test_num in range(20):
        # Create synthetic input matching the expected shape [1, 1, 32, 32]
        input_tensor = torch.randn(1, 1, 32, 32)
        
        with torch.no_grad():
            output = model(input_tensor)
            predicted = torch.argmax(output).item()
            confidence = torch.nn.functional.softmax(output, dim=1)[0][predicted].item()
        
        # Use modulo to ensure we have a valid index
        class_idx = predicted % len(target_classes)
        
        test_results.append({
            'test_id': test_num,
            'prediction': predicted,
            'confidence': round(confidence, 4),
            'label': target_classes[class_idx]
        })
    
    # Analyze results
    successful = len([r for r in test_results if r['confidence'] > 0.15])
    avg_confidence = sum(r['confidence'] for r in test_results) / len(test_results) if test_results else 0
    
    validation_report = {
        'total_tests': len(test_results),
        'passed_tests': successful,
        'pass_rate': successful / len(test_results) if test_results else 0,
        'avg_confidence': avg_confidence,
        'validation_status': 'PASS' if successful >= 15 else 'FAIL',
        'test_details': test_results,
        'model_source': model_path,
        'target_classes': target_classes
    }
    
    # Save results
    os.makedirs('model-output', exist_ok=True)
    with open('model-output/validation-report.json', 'w') as f:
        json.dump(validation_report, f, indent=2)
    
    # Save CSV data
    with open('model-output/validation-data.csv', 'w') as csv_file:
        csv_file.write('test_id,prediction,confidence,label\n')
        for result in test_results:
            csv_file.write(f'{result["test_id"]},{result["prediction"]},{result["confidence"]},{result["label"]}\n')
    
    print(f'Validation complete!')
    print(f'Success rate: {validation_report["pass_rate"]:.1%}')
    print(f'Average confidence: {validation_report["avg_confidence"]:.3f}')
    print(f'Status: {validation_report["validation_status"]}')
    
except Exception as e:
    print(f'Error during validation: {e}')
    import traceback
    traceback.print_exc()
    
    # Create a basic validation report even if validation fails
    validation_report = {
        'total_tests': 0,
        'passed_tests': 0,
        'pass_rate': 0,
        'avg_confidence': 0,
        'validation_status': 'ERROR',
        'error': str(e),
        'model_source': model_path if 'model_path' in locals() else 'unknown'
    }
    
    os.makedirs('model-output', exist_ok=True)
    with open('model-output/validation-report.json', 'w') as f:
        json.dump(validation_report, f, indent=2)
    
    sys.exit(1)
EOF
        
        python3 validate_model.py
    
    - name: Save validation results
      uses: actions/upload-artifact@v4
      with:
        name: validation-${{ github.run_id }}
        path: |
          model-output/validation-report.json
          model-output/validation-data.csv
        retention-days: 28

  # Job 3: Performance evaluation
  evaluate-performance:
    name: Evaluate Model Performance
    runs-on: ubuntu-22.04
    needs: [train-model, validate-model]
    if: needs.train-model.outputs.training_status == 'success'
    
    steps:
    - name: Download repository
      uses: actions/checkout@v4
    
    - name: Get model artifacts
      uses: actions/download-artifact@v4
      with:
        name: ${{ env.ARTIFACT_NAME }}
        path: model-output/
    
    - name: Copy model files to root for inference server
      run: |
        echo "Copying model files for inference..."
        cp model-output/model.pth . 2>/dev/null || true
        cp model-output/class_info.json . 2>/dev/null || true
        cp model-output/training_metrics.json . 2>/dev/null || true
    
    - name: Build inference server
      run: |
        echo "Building inference server..."
        docker build -f Dockerfile.inference -t model-server:latest .
    
    - name: Install tools
      run: |
        echo "Installing performance tools..."
        pip install requests soundfile
    
    - name: Run performance test
      run: |
        echo "Running performance evaluation..."
        
        # Start server in background
        docker run -d --name test-server -p 5000:5000 model-server:latest
        
        # Wait for startup with retries
        echo "Waiting for server to start..."
        for i in {1..10}; do
          if curl -s http://localhost:5000/health > /dev/null; then
            echo "Server is up!"
            break
          fi
          echo "Waiting... ($i/10)"
          sleep 5
        done
        
        # Check if server is actually running
        if ! curl -s http://localhost:5000/health > /dev/null; then
          echo "ERROR: Server failed to start"
          docker logs test-server || true
          exit 1
        fi
        
        # Create a simple performance test script
        cat > performance_test.py << 'EOF'
    import requests
    import time
    import json
    import io
    import numpy as np
    import soundfile as sf

def generate_test_audio():
    """Generate simple test audio"""
    sample_rate = 16000
    duration = 1.0
    t = np.linspace(0, duration, int(sample_rate * duration))
    audio_data = 0.5 * np.sin(2 * np.pi * 440 * t)
    
    buffer = io.BytesIO()
    sf.write(buffer, audio_data, sample_rate, format='WAV')
    buffer.seek(0)
    return buffer

print('Starting performance tests...')
response_times = []

# Test health endpoint first
health_start = time.time()
try:
    health_resp = requests.get('http://localhost:5000/health', timeout=10)
    health_time = (time.time() - health_start) * 1000
    print(f'Health check: {health_time:.2f}ms (HTTP {health_resp.status_code})')
except Exception as e:
    print(f'Health check failed: {e}')

# Test prediction endpoint
for i in range(5):  # Reduced number of requests for CI/CD
    try:
        # Generate test audio
        audio_buffer = generate_test_audio()
        
        files = {'file': ('test.wav', audio_buffer, 'audio/wav')}
        
        start = time.time()
        response = requests.post('http://localhost:5000/predict', files=files, timeout=30)
        end = time.time()
        
        if response.status_code == 200:
            latency = (end - start) * 1000
            response_times.append(latency)
            result = response.json()
            print(f'Request {i+1}: {latency:.2f}ms -> {result.get("prediction", "unknown")}')
        else:
            print(f'Request {i+1}: HTTP {response.status_code}')
            
    except Exception as e:
        print(f'Request {i+1} failed: {e}')

# Calculate metrics
if response_times:
    import statistics
    performance_data = {
        'total_requests': 5,
        'successful': len(response_times),
        'success_rate': len(response_times) / 5,
        'avg_response_ms': statistics.mean(response_times),
        'min_response_ms': min(response_times),
        'max_response_ms': max(response_times),
        'p95_ms': sorted(response_times)[-1] if response_times else 0  # Simplified for small sample
    }
    
    print(f'Performance test complete!')
    print(f'Success rate: {performance_data["success_rate"]:.1%}')
    print(f'Average response: {performance_data["avg_response_ms"]:.2f}ms')
else:
    performance_data = {
        'error': 'No successful requests',
        'total_requests': 5,
        'successful': 0,
        'success_rate': 0
    }
    print('Performance test failed - no successful requests')

# Save results
with open('performance-report.json', 'w') as f:
    json.dump(performance_data, f, indent=2)

# Save raw data
if response_times:
    with open('performance-data.csv', 'w') as csv_file:
        csv_file.write('request,response_time_ms\n')
        for idx, time_val in enumerate(response_times, 1):
            csv_file.write(f'{idx},{time_val:.2f}\n')
EOF
        
        python3 performance_test.py
        
        # Cleanup
        docker stop test-server 2>/dev/null || true
        docker rm test-server 2>/dev/null || true
    
    - name: Save performance results
      uses: actions/upload-artifact@v4
      with:
        name: performance-${{ github.run_id }}
        path: |
          performance-report.json
          performance-data.csv
        retention-days: 28

  # Job 4: Container publication
  publish-container:
    name: Publish Container
    runs-on: ubuntu-22.04
    needs: [train-model, validate-model, evaluate-performance]
    if: |
      (github.event_name == 'push' && github.ref == 'refs/heads/main') ||
      (github.event_name == 'push' && github.ref == 'refs/heads/primary') ||
      (github.event_name == 'workflow_dispatch' && github.event.inputs.upload_to_registry == 'true')
    
    permissions:
      contents: read
      packages: write
    
    steps:
    - name: Download repository
      uses: actions/checkout@v4
    
    - name: Get model artifacts
      uses: actions/download-artifact@v4
      with:
        name: ${{ env.ARTIFACT_NAME }}
        path: model-output/
    
    - name: Copy model files to root for Docker build
      run: |
        echo "Preparing model files for Docker build..."
        cp model-output/model.pth . 2>/dev/null || true
        cp model-output/class_info.json . 2>/dev/null || true
        cp model-output/training_metrics.json . 2>/dev/null || true
        
        echo "Current directory:"
        ls -la
    
    - name: Normalize repository identifier
      id: normalize-name
      run: |
        NORMALIZED=$(echo "${{ github.repository }}" | tr '[:upper:]' '[:lower:]')
        echo "repo-name=$NORMALIZED" >> $GITHUB_OUTPUT
        echo "Normalized name: $NORMALIZED"
    
    - name: Login to registry
      uses: docker/login-action@v3
      with:
        registry: ${{ env.REGISTRY_URL }}
        username: ${{ github.actor }}
        password: ${{ secrets.GITHUB_TOKEN }}
    
    - name: Build and publish
      run: |
        echo "Building and publishing container..."
        
        REPO_NAME="${{ steps.normalize-name.outputs.repo-name }}"
        MODEL_TAG="${{ needs.train-model.outputs.model_version }}"
        
        echo "Repository: $REPO_NAME"
        echo "Model version: $MODEL_TAG"
        
        # Build container with inference Dockerfile
        docker build -f Dockerfile.inference \
          -t ${{ env.REGISTRY_URL }}/$REPO_NAME:latest \
          -t ${{ env.REGISTRY_URL }}/$REPO_NAME:$MODEL_TAG .
        
        # Publish
        echo "Publishing to registry..."
        docker push ${{ env.REGISTRY_URL }}/$REPO_NAME:latest
        docker push ${{ env.REGISTRY_URL }}/$REPO_NAME:$MODEL_TAG
        
        echo "Container published successfully"
        echo "Latest: ${{ env.REGISTRY_URL }}/$REPO_NAME:latest"
        echo "Versioned: ${{ env.REGISTRY_URL }}/$REPO_NAME:$MODEL_TAG"
        echo "Model Accuracy: ${{ needs.train-model.outputs.performance_score }}%"
    
    - name: Generate summary
      run: |
        echo "SUMMARY<<EOF" >> $GITHUB_ENV
        echo "## Deployment Complete" >> $GITHUB_ENV
        echo "" >> $GITHUB_ENV
        echo "**Published Containers:**" >> $GITHUB_ENV
        echo "- \`${{ env.REGISTRY_URL }}/${{ steps.normalize-name.outputs.repo-name }}:latest\`" >> $GITHUB_ENV
        echo "- \`${{ env.REGISTRY_URL }}/${{ steps.normalize-name.outputs.repo-name }}:${{ needs.train-model.outputs.model_version }}\`" >> $GITHUB_ENV
        echo "" >> $GITHUB_ENV
        echo "**Model Metrics:**" >> $GITHUB_ENV
        echo "- Accuracy: ${{ needs.train-model.outputs.performance_score }}%" >> $GITHUB_ENV
        echo "- Version: ${{ needs.train-model.outputs.model_version }}" >> $GITHUB_ENV
        EOF" >> $GITHUB_ENV

  # Job 5: Final report
  generate-report:
    name: Generate Final Report
    runs-on: ubuntu-22.04
    needs: [train-model, validate-model, evaluate-performance, publish-container]
    if: always()
    
    steps:
    - name: Download repository
      uses: actions/checkout@v4
    
    - name: Create report
      run: |
        echo "Generating pipeline report..."
        
        cat > pipeline-report.md << EOF
        # ML Pipeline Execution Report
        
        ## Execution Details
        - **Run ID**: ${{ github.run_id }}
        - **Trigger**: ${{ github.event_name }}
        - **Commit**: ${{ github.sha }}
        - **Branch**: ${{ github.ref }}
        - **Time**: $(date -u)
        - **Model Version**: ${{ needs.train-model.outputs.model_version || 'N/A' }}
        
        ## Stage Results
        | Stage | Status | Details |
        |-------|--------|---------|
        | Model Training | ${{ needs.train-model.result }} | Accuracy: ${{ needs.train-model.outputs.performance_score || 'N/A' }}% |
        | Validation | ${{ needs.validate-model.result }} | Quality checks completed |
        | Performance | ${{ needs.evaluate-performance.result }} | Response time analysis |
        | Container Publish | ${{ needs.publish-container.result || 'SKIPPED' }} | ${{ needs.publish-container.result == 'success' && 'Published to registry' || 'Not published' }} |
        
        ## Generated Outputs
        1. Model files: model.pth, model_full.pth
        2. Training logs: training.log
        3. Validation data: validation-report.json, validation-data.csv
        4. Performance data: performance-report.json, performance-data.csv
        ${{ needs.publish-container.result == 'success' && '5. Container images: Published to registry' || '' }}
        
        ## Quality Checks
        - Training completion: ${{ needs.train-model.outputs.training_status == 'success' && 'PASS' || 'FAIL' }}
        - Validation: ${{ needs.validate-model.result == 'success' && 'PASS' || 'FAIL' }}
        - Performance: ${{ needs.evaluate-performance.result == 'success' && 'PASS' || 'FAIL' }}
        - Deployment: ${{ needs.publish-container.result == 'success' && 'PASS' || 'SKIPPED' }}
        
        ## Pipeline Features
        - Multi-stage container builds
        - Isolated training environment
        - Artifact management
        - Quality validation
        - Performance testing
        - Registry integration
        
        EOF
        
        echo "Report generated successfully"
    
    - name: Save report
      uses: actions/upload-artifact@v4
      with:
        name: report-${{ github.run_id }}
        path: pipeline-report.md
        retention-days: 60

  # Job 6: Quality check
  quality-check:
    name: Quality Verification
    runs-on: ubuntu-22.04
    needs: [train-model, validate-model, evaluate-performance]
    if: |
      (github.event_name == 'pull_request') ||
      (github.event_name == 'workflow_dispatch')
    
    steps:
    - name: Check status
      run: |
        echo "Verifying quality gates..."
        
        TRAIN_RESULT="${{ needs.train-model.result }}"
        VALIDATE_RESULT="${{ needs.validate-model.result }}"
        PERF_RESULT="${{ needs.evaluate-performance.result }}"
        
        echo "Training: $TRAIN_RESULT"
        echo "Validation: $VALIDATE_RESULT"
        echo "Performance: $PERF_RESULT"
        
        if [ "$TRAIN_RESULT" = "success" ] && \
           [ "$VALIDATE_RESULT" = "success" ] && \
           [ "$PERF_RESULT" = "success" ]; then
          echo "All quality checks passed"
          echo "## Quality Check Passed" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "All stages completed successfully:" >> $GITHUB_STEP_SUMMARY
          echo "- Model Training: Success" >> $GITHUB_STEP_SUMMARY
          echo "- Model Validation: Success" >> $GITHUB_STEP_SUMMARY
          echo "- Performance Test: Success" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          if [ "${{ github.event_name }}" = "pull_request" ]; then
            echo "**This change meets quality requirements.**" >> $GITHUB_STEP_SUMMARY
          fi
          
          exit 0
        else
          echo "Quality check failed"
          echo "## Quality Check Failed" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "Failed stages:" >> $GITHUB_STEP_SUMMARY
          [ "$TRAIN_RESULT" != "success" ] && echo "- Training: $TRAIN_RESULT" >> $GITHUB_STEP_SUMMARY
          [ "$VALIDATE_RESULT" != "success" ] && echo "- Validation: $VALIDATE_RESULT" >> $GITHUB_STEP_SUMMARY
          [ "$PERF_RESULT" != "success" ] && echo "- Performance: $PERF_RESULT" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          if [ "${{ github.event_name }}" = "pull_request" ]; then
            echo "**This change requires all checks to pass.**" >> $GITHUB_STEP_SUMMARY
          fi
          
          exit 1
        fi